Project Overview
===============

Project Statistics:
Total Files: 8
Total Size: 17.24 KB

File Types:
  .ts: 5 files
  .json: 2 files
  no extension: 1 files

Detected Technologies:
  - TypeScript

Folder Structure (Tree)
=====================
Legend: ✓ = Included in output, ✗ = Excluded from output

├── .gitignore (36 B) ✓
├── package.json (461 B) ✓
├── src/
│   ├── kafka.ts (2.32 KB) ✓
│   ├── main.ts (11.61 KB) ✓
│   ├── state.ts (899 B) ✓
│   ├── types-ext.d.ts (1.02 KB) ✓
│   └── types.ts (746 B) ✓
└── tsconfig.json (205 B) ✓

==============

File Name: .gitignore
Size: 36 B
Code:
node_modules
package-lock.json
dist

-------- [ Separator ] ------

File Name: package.json
Size: 461 B
Code:
{
  "name": "cladbe-ws-gateway",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "tsx src/main.ts",
    "build": "tsc -p tsconfig.json",
    "start": "node dist/main.js"
  },
  "dependencies": {
    "node-rdkafka": "^2.18.0",
    "uuid": "^9.0.1",
    "uWebSockets.js": "uNetworking/uWebSockets.js#v20.48.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/node": "^20.11.30",
    "tsx": "^4.7.0",
    "typescript": "^5.4.5"
  }
}

-------- [ Separator ] ------

File Name: src/kafka.ts
Size: 2.32 KB
Code:
// src/kafka.ts
/* eslint-disable @typescript-eslint/no-explicit-any */
import { KafkaConsumer, LibrdKafkaError, Message } from 'node-rdkafka';

export type KafkaHandlers = {
    onMessage: (key: string, value: Buffer, raw: Message) => void;
    onError?: (err: LibrdKafkaError) => void;
    onRebalance?: (ev: any) => void;
};

export class GatewayConsumer {
    private consumer: KafkaConsumer;
    private paused = false;

    constructor(
        private topics: string[],
        private groupId: string,
        private brokers: string,
        private handlers: KafkaHandlers,
    ) {
        this.consumer = new KafkaConsumer(
            {
                'metadata.broker.list': brokers,
                'group.id': groupId,
                'enable.auto.commit': true,
                'socket.keepalive.enable': true,
                // throughput tuning
                'fetch.wait.max.ms': 50,
                'fetch.min.bytes': 65536,              // 64 KiB
                'queued.max.messages.kbytes': 102400,  // 100 MiB
                'allow.auto.create.topics': true,
                'client.id': 'cladbe-ws-gateway',
            },
            { 'auto.offset.reset': 'latest' }
        );
    }

    start() {
        this.consumer
            .on('ready', () => {
                this.consumer.subscribe(this.topics);
                this.consumer.consume();
            })
            .on('data', (m) => {
                const key = m.key
                    ? (Buffer.isBuffer(m.key) ? m.key.toString('utf8') : String(m.key))
                    : '';
                if (!key || !m.value) return;
                this.handlers.onMessage(key, m.value, m);
            })
            .on('event.error', (err) => this.handlers.onError?.(err))
            .on('rebalance', (ev) => this.handlers.onRebalance?.(ev));

        this.consumer.connect();
    }

    pauseAll() {
        if (this.paused) return;
        const asg = this.consumer.assignments();
        if (asg.length) {
            this.consumer.pause(asg);
            this.paused = true;
        }
    }

    resumeAll() {
        if (!this.paused) return;
        const asg = this.consumer.assignments();
        if (asg.length) {
            this.consumer.resume(asg);
            this.paused = false;
        }
    }

    stop() {
        try { this.consumer.disconnect(); } catch {}
    }
}
-------- [ Separator ] ------

File Name: src/main.ts
Size: 11.61 KB
Code:
import uWS from "uWebSockets.js";
import { randomUUID } from "node:crypto";
import { z } from "zod";
import { sessions, addSub, removeSub, subToSessions } from "./state.js";
import { type ClientMsg, type ServerMsg, subKey } from "./types.js";
import { GatewayConsumer } from "./kafka.js";

const PORT = Number(process.env.WS_PORT || 8080);
const PING_INTERVAL_MS = 25_000;
const MAX_QUEUE = 1000; // frames per connection

// Kafka config (filtered CDC fan-out)
const KAFKA_BROKERS = process.env.KAFKA_BROKERS || "localhost:9092";
const KAFKA_GROUP   = process.env.KAFKA_GROUP   || "cladbe-ws-gateway";
const KAFKA_TOPICS  = (process.env.KAFKA_TOPICS || "server.cdc.filtered").split(",");

// ---------------- LSN support (headers-only; no payload decoding) ----------------
type SubState = {
    cursorLsn: bigint;   // watermark for this subscription
    buffering: boolean;  // true until snapshot sent
    buffer: Array<{ lsn: bigint; payload: Buffer }>;
};

let LAST_SEEN_LSN: bigint = 0n;

// node-rdkafka headers arrive as Array<{ key: string, value: Buffer|string|null }>
function readLsnHeader(raw: any): bigint {
    const hs: Array<{ key: string; value: any; }> | undefined = (raw as any).headers;
    if (!hs || !Array.isArray(hs)) return 0n;
    const h = hs.find(x => x && x.key === 'lsn');
    if (!h || !h.value) return 0n;
    const v = h.value as Buffer | string;
    const buf = Buffer.isBuffer(v) ? v : Buffer.from(String(v), 'binary');
    if (buf.length !== 8) return 0n;
    return buf.readBigUInt64BE(0);
}

// -------------------------------------------------------------------------------

// global backpressure tracking for coarse consumer flow control
let SLOW_SOCKETS = 0;

function asArrayBuffer(buf: Buffer): ArrayBuffer {
    return buf.buffer.slice(buf.byteOffset, buf.byteOffset + buf.byteLength) as ArrayBuffer;
}

function parseSubprotocol(req: uWS.HttpRequest) {
    const raw = req.getHeader("sec-websocket-protocol") || "";
    const parts = raw.split(",").map(s => s.trim()).filter(Boolean);
    const chosen = parts[0] || "json";
    const bearer = parts.find(p => p.startsWith("bearer."));
    const token = bearer ? bearer.slice("bearer.".length) : undefined;
    return { chosen, token };
}

// --- minimal auth stub ---
function authenticate(req: uWS.HttpRequest) {
    const { chosen, token } = parseSubprotocol(req);
    // TODO: verify JWT properly (e.g., jose/jwt) and extract claims.sub as userId
    // For now, accept token if present, else allow anon
    const tenantId = req.getHeader("x-tenant") || "demo";
    const userId = token && token.length > 0 ? token : (req.getHeader("x-user") || "anon");
    return { ok: true, userId, tenantId, chosen };
}

function safeSend(s: uWS.WebSocket<any>, msg: ServerMsg) {
    const buf = JSON.stringify(msg);
    const st = sessions.get((s as any).id);
    if (!st) return;

    // Fast path
    const wrote = s.send(buf);
    if (wrote) return;

    // Socket buffered: queue JSON fallback
    (st as any).sendQueue.push(buf);
    if (!(st as any)._slow) { (st as any)._slow = true; SLOW_SOCKETS++; }
    if ((st as any).sendQueue.length > MAX_QUEUE) {
        (st as any).sendQueue.length = 0;
        s.send(JSON.stringify({ op: "error", code: "overflow", message: "reset-to-snapshot" }));
    }
}

// LSN-aware delivery: buffer during snapshot, gate after by cursor
function deliverBinaryLSN(hashId: string, payload: Buffer, lsn: bigint, onlySession?: any) {
    const targetSessions = onlySession ? [onlySession] : [...sessions.values()];
    let delivered = 0;

    for (const st of targetSessions) {
        // Iterate this session's subs and find those matching this hashId
        for (const key of st.subs) {
            if (!key.endsWith(hashId)) continue; // subKey currently = hashId

            const subStates: Map<string, SubState> = st.subStates ?? new Map();
            const sub = subStates.get(key);
            if (!sub) continue;

            if (sub.buffering) {
                sub.buffer.push({ lsn, payload });
                continue;
            }
            if (lsn <= sub.cursorLsn) continue;

            // send as binary
            const ok = st.socket.send(asArrayBuffer(payload), true, false);
            if (ok) {
                sub.cursorLsn = lsn;
                delivered++;
            } else {
                // backpressure → JSON fallback (base64)
                const b64 = payload.toString("base64");
                st.sendQueue.push(JSON.stringify({ op: "diffB64", hashId, b64 }));
                if (!st._slow) { st._slow = true; SLOW_SOCKETS++; }
                if (st.sendQueue.length > MAX_QUEUE) {
                    st.sendQueue.length = 0;
                    st.socket.send(JSON.stringify({ op: "error", code: "overflow", message: "reset-to-snapshot" }));
                }
            }
        }
    }
    return delivered;
}

const subscribeSchema = z.object({
    op: z.literal("subscribe"),
    table: z.string().min(1),
    hashId: z.string().min(1),
    queryFbB64: z.string().min(1),
    resumeFromVersion: z.number().int().nonnegative().optional()
});

const unsubscribeSchema = z.object({
    op: z.literal("unsubscribe"),
    table: z.string().min(1),
    hashId: z.string().min(1)
});

uWS.App({})
    .ws("/*", {
        idleTimeout: 60,
        maxBackpressure: 1 << 20, // 1 MiB per socket
        maxPayloadLength: 1 << 20,

        upgrade: (res, req, context) => {
            const auth = authenticate(req);
            if (!auth.ok) {
                res.writeStatus("401 Unauthorized").end();
                return;
            }
            // Echo the selected subprotocol so browsers accept the handshake
            if (auth.chosen) {
                res.writeHeader("Sec-WebSocket-Protocol", auth.chosen);
            }
            res.upgrade(
                { userId: auth.userId, tenantId: auth.tenantId },
                req.getHeader("sec-websocket-key"),
                req.getHeader("sec-websocket-protocol"),
                req.getHeader("sec-websocket-extensions"),
                context
            );
        },

        open: (ws) => {
            const id = randomUUID();
            (ws as any).id = id;
            const s = {
                id,
                socket: ws,
                userId: (ws as any).userId,
                tenantId: (ws as any).tenantId,
                subs: new Set<string>(),
                sendQueue: [] as string[],
            } as any;

            // NEW: per-session subscription state (hashId/table-key -> SubState)
            s.subStates = new Map<string, SubState>();

            sessions.set(id, s);

            // heartbeat
            const interval = setInterval(() => {
                try { ws.ping(); } catch { /* ignore */ }
            }, PING_INTERVAL_MS);
            (ws as any)._heartbeat = interval;
        },

        message: (ws, arrayBuffer, isBinary) => {
            if (isBinary) return; // client should send JSON
            let msg: ClientMsg;
            try {
                msg = JSON.parse(Buffer.from(arrayBuffer).toString("utf8"));
            } catch {
                safeSend(ws, { op: "error", code: "bad_json", message: "invalid JSON" });
                return;
            }

            if (msg.op === "ping") { safeSend(ws, { op: "pong" }); return; }

            // subscribe (LSN-fenced)
            if (subscribeSchema.safeParse(msg).success) {
                const { table, hashId, resumeFromVersion } = msg as any;
                const key = subKey(table, hashId); // = hashId (routing key)
                const s = sessions.get((ws as any).id)!;

                // TODO(tenant safety): enforce tenant predicate injection up-front.

                addSub(s, key);
                safeSend(ws, { op: "ack", hashId });

                // 1) Fence at current global LSN (or client's resume if higher)
                let fence = LAST_SEEN_LSN;
                if (typeof resumeFromVersion === 'number' && Number.isFinite(resumeFromVersion)) {
                    const r = BigInt(resumeFromVersion);
                    if (r > fence) fence = r;
                }

                // 2) Create subState in buffering mode
                (s as any).subStates.set(key, { cursorLsn: fence, buffering: true, buffer: [] });

                // 3) Snapshot (placeholder; integrate SQL-RPC here)
                const snapshotRows: any[] = []; // TODO: replace with actual SQL RPC rows
                safeSend(ws, {
                    op: "snapshot",
                    hashId,
                    version: 0,
                    cursor: { lsn: fence.toString() },
                    rows: snapshotRows
                });

                // 4) Flush buffered diffs strictly newer than fence
                const sub = (s as any).subStates.get(key) as SubState;
                if (sub) {
                    sub.buffer.sort((a, b) => (a.lsn < b.lsn ? -1 : (a.lsn > b.lsn ? 1 : 0)));
                    for (const m of sub.buffer) {
                        if (m.lsn > sub.cursorLsn) {
                            deliverBinaryLSN(hashId, m.payload, m.lsn, s);
                            sub.cursorLsn = m.lsn;
                        }
                    }
                    sub.buffer = [];
                    sub.buffering = false;
                }
                return;
            }

            // unsubscribe
            if (unsubscribeSchema.safeParse(msg).success) {
                const { table, hashId } = msg as any;
                const key = subKey(table, hashId);
                const s = sessions.get((ws as any).id)!;
                removeSub(s, key);
                (s as any).subStates?.delete(key);
                return;
            }

            safeSend(ws, { op: "error", code: "bad_op", message: "unknown message" });
        },

        drain: (ws) => {
            const s = sessions.get((ws as any).id);
            if (!s) return;

            // socket writable again
            if ((s as any)._slow) {
                (s as any)._slow = false;
                if (SLOW_SOCKETS > 0) SLOW_SOCKETS--;
            }

            while (s.sendQueue.length) {
                const next = s.sendQueue.shift()!;
                const ok = ws.send(next);
                if (!ok) { s.sendQueue.unshift(next); break; }
            }
        },

        close: (ws) => {
            clearInterval((ws as any)._heartbeat);
            const s = sessions.get((ws as any).id);
            if (s) {
                for (const key of [...s.subs]) removeSub(s, key);
                (s as any).subStates?.clear?.();
                if ((s as any)._slow) { (s as any)._slow = false; if (SLOW_SOCKETS > 0) SLOW_SOCKETS--; }
                sessions.delete(s.id);
            }
        }
    })
    .any("/*", (res, _req) => void res.writeStatus("200 OK").end("cladbe-ws-gateway"))
    .listen(PORT, (ok) => {
        if (!ok) { console.error("WS listen failed"); process.exit(1); }
        console.log(`WS listening on :${PORT}`);
    });

// ---- Kafka consumer → fan-out ----
const consumer = new GatewayConsumer(
    KAFKA_TOPICS,
    KAFKA_GROUP,
    KAFKA_BROKERS,
    {
        onMessage: (hashId, value /* Buffer */, raw) => {
            // Read LSN header and keep a global watermark
            const lsn = readLsnHeader(raw);
            if (lsn > LAST_SEEN_LSN) LAST_SEEN_LSN = lsn;

            // Deliver with LSN gating/buffering
            deliverBinaryLSN(hashId, value, lsn);
        },
        onError: (err) => console.error("[kafka] error", err),
        onRebalance: (ev) => console.log("[kafka] rebalance", ev?.code ?? ev),
    }
);
consumer.start();

// coarse flow control: pause/resume if too many slow sockets
setInterval(() => {
    if (SLOW_SOCKETS > 100) consumer.pauseAll();
    else consumer.resumeAll();
}, 250);
-------- [ Separator ] ------

File Name: src/state.ts
Size: 899 B
Code:
import type { SubKey } from "./types";

export interface Session {
    id: string;
    socket: any;             // uWS.WebSocket
    userId: string;
    tenantId: string;
    subs: Set<SubKey>;
    sendQueue: string[];     // backpressure buffer (serialized frames)
};

export const sessions = new Map<string, Session>();

// Subscriptions: subKey -> Set(sessionId)
export const subToSessions = new Map<SubKey, Set<string>>();

export function addSub(s: Session, key: SubKey) {
    if (!s.subs.has(key)) {
        s.subs.add(key);
        if (!subToSessions.has(key)) subToSessions.set(key, new Set());
        subToSessions.get(key)!.add(s.id);
    }
}

export function removeSub(s: Session, key: SubKey) {
    if (s.subs.delete(key)) {
        const g = subToSessions.get(key);
        if (g) {
            g.delete(s.id);
            if (g.size === 0) subToSessions.delete(key);
        }
    }
}
-------- [ Separator ] ------

File Name: src/types-ext.d.ts
Size: 1.02 KB
Code:
// declare module 'node-rdkafka' {
//     // Minimal surface we use; you can replace with community types
//     export interface Message {
//         value?: Buffer;
//         key?: Buffer | string | null;
//         topic: string;
//         partition: number;
//         offset: number;
//         timestamp: number;
//     }
//     export interface LibrdKafkaError extends Error {
//         code: number;
//     }
//     export class KafkaConsumer {
//         constructor(globalConf: any, topicConf?: any);
//         connect(): void;
//         disconnect(): void;
//         subscribe(topics: string[]): void;
//         consume(): void;
//         assignments(): any[];
//         pause(assignments: any[]): void;
//         resume(assignments: any[]): void;
//         on(event: 'ready', cb: () => void): this;
//         on(event: 'data', cb: (message: Message) => void): this;
//         on(event: 'rebalance', cb: (ev: any) => void): this;
//         on(event: 'event.error', cb: (err: LibrdKafkaError) => void): this;
//     }
// }
-------- [ Separator ] ------

File Name: src/types.ts
Size: 746 B
Code:
export type ClientMsg =
    | { op: "ping" }
    | { op: "subscribe"; table: string; hashId: string; queryFbB64: string; resumeFromVersion?: number }
    | { op: "unsubscribe"; table: string; hashId: string };

export type ServerMsg =
    | { op: "pong" }
    | { op: "ack"; hashId: string }
    | { op: "snapshot"; hashId: string; version: number; cursor: Record<string, any>; rows: any[] }
    | { op: "diff"; hashId: string; version: number; cursor: Record<string, any>; changes: any[] }
    | { op: "diffB64"; hashId: string; b64: string }
    | { op: "error"; code: string; message: string };

export type SubKey = string; // `${table}|${hashId}`
export const subKey = (_table: string, hashId: string) => hashId; // <— route by hashId only
-------- [ Separator ] ------
