Project Overview
===============

Project Statistics:
Total Files: 10
Total Size: 28.12 KB

File Types:
  .ts: 6 files
  .json: 2 files
  .fbs: 2 files

Detected Technologies:
  - TypeScript

Folder Structure (Tree)
=====================
Legend: ✓ = Included in output, ✗ = Excluded from output

├── package.json (610 B) ✓
├── schemas/
│   └── sql/
│       ├── sql_rpc.fbs (2.69 KB) ✓
│       └── sql_schema.fbs (5.12 KB) ✓
├── src/
│   ├── index.ts (162 B) ✓
│   ├── rpc/
│   │   ├── fb_decode.ts (4.64 KB) ✓
│   │   ├── fb_maps.ts (656 B) ✓
│   │   ├── kafka.ts (3.68 KB) ✓
│   │   └── worker.ts (9.98 KB) ✓
│   └── types/
│       └── env.d.ts (364 B) ✓
└── tsconfig.json (268 B) ✓

==============

File Name: package.json
Size: 610 B
Code:
{
  "name": "@cladbe/postgres_rpc",
  "version": "1.0.0",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "private": true,
  "scripts": {
    "build": "tsc",
    "gen:ts": "flatc --ts -o src/rpc/generated schemas/sql/*.fbs",
    "dev": "node --loader ts-node/esm src/index.ts",
    "start": "node dist/index.js"
  },
  "dependencies": {
    "@cladbe/postgres_manager": "file:../postgres_manager",
    "dotenv": "^17.2.1",
    "flatbuffers": "^23.5.26",
    "node-rdkafka": "^3.5.0"
  },
  "devDependencies": {
    "@types/node": "^24.1.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.5.3"
  }
}

-------- [ Separator ] ------

File Name: schemas/sql/sql_rpc.fbs
Size: 2.69 KB
Code:
include "sql_schema.fbs"; // your earlier FBS with explicit numeric enums
namespace SqlRpc;

// ---------- RPC Method ----------
enum RpcMethod : ubyte {
  GET_DATA         = 0,
  GET_SINGLE       = 1,
  ADD_SINGLE       = 2,
  UPDATE_SINGLE    = 3,
  DELETE_ROW       = 4,
  CREATE_TABLE     = 5,
  TABLE_EXISTS     = 6,
  RUN_AGGREGATION  = 7
}

// ---------- Requests (use pieces from SqlSchema) ----------

table GetDataReq {
  company_id: string;
  table_name: string;
  wrapper: SqlSchema.BasicSqlDataFilterWrapper;  // optional; for simple calls pass empty
  limit: uint32 = 0;      // 0 → no limit
  offset: uint32 = 0;     // optional; prefer cursor later
  order: [SqlSchema.OrderKeySpec];               // optional
  cursor: [SqlSchema.CursorEntry];
  strict_after: bool = true;
}

table GetSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table AddSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  // row as JSON (server already casts objects to jsonb), pragmatic & compact
  row_json: string;
}

table UpdateSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
  updates_json: string;
}

table DeleteRowReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table CreateTableReq {
  company_id: string;
  definition: SqlSchema.TableDefinition;
}

table TableExistsReq {
  company_id: string;
  table_name: string;
}

table RunAggregationReq {
  company_id: string;
  table_name: string;
  count_enabled: bool = false;
  sum_fields: [string];
  average_fields: [string];
  minimum_fields: [string];
  maximum_fields: [string];
  wrapper: SqlSchema.BasicSqlDataFilterWrapper;
}

// ---------- Responses ----------

table RowsJson { rows: [string]; }           // each element is one row JSON
table RowJson  { row: string; }              // single row JSON
table BoolRes  { value: bool; }
table AggRes   { agg: SqlSchema.DataHelperAggregation; }

enum ErrorCode : ubyte { NONE = 0, BAD_REQUEST = 1, INTERNAL = 2 }

union RpcResponse {
  RowsJson, RowJson, BoolRes, AggRes
}

table ResponseEnvelope {
  correlation_id: string;
  ok: bool = true;
  error_code: ErrorCode = NONE;
  error_message: string;
  data: RpcResponse;
}

// ---------- Envelope ----------

union RpcPayload {
  GetDataReq, GetSingleReq, AddSingleReq, UpdateSingleReq, DeleteRowReq,
  CreateTableReq, TableExistsReq, RunAggregationReq
}

table RequestEnvelope {
  correlation_id: string;
  reply_topic: string;        // producer can override default reply topic
  method: RpcMethod;
  payload: RpcPayload;        // one of the above
}

root_type RequestEnvelope;

-------- [ Separator ] ------

File Name: schemas/sql/sql_schema.fbs
Size: 5.12 KB
Code:
namespace SqlSchema;

// ---------------- Enums with explicit numbers ----------------

// Stable: text/numeric types
enum SQLDataType : ubyte {
  text             = 0,
  varchar          = 1,
  char_            = 2,
  varcharArray     = 3,
  textArray        = 4,
  charArray        = 5,
  integer          = 6,
  bigInt           = 7,
  smallInt         = 8,
  decimal          = 9,
  numeric          = 10,
  real             = 11,
  doublePrecision  = 12,
  serial           = 13,
  bigSerial        = 14,
  smallSerial      = 15,
  money            = 16,
  date             = 17,
  time             = 18,
  timestamp        = 19,
  timestamptz      = 20,
  interval         = 21,
  timetz           = 22,
  boolean_         = 23,
  bytea            = 24,
  json             = 25,
  jsonb            = 26,
  jsonArray        = 27,
  jsonbArray       = 28,
  uuid             = 29,
  xml              = 30,
  array            = 31,
  custom           = 32
}

enum ColumnConstraint : ubyte {
  primaryKey        = 0,
  unique            = 1,
  notNull           = 2,
  check             = 3,
  default_          = 4,
  indexed           = 5,
  exclusion         = 6,
  generated         = 7,
  identity          = 8,
  references        = 9,
  noInherit         = 10,
  nullsNotDistinct  = 11
}

enum SQLFilterWrapperType : ubyte {
  or   = 0,
  and  = 1
}

enum NullsSortOrder : ubyte {
  first   = 0,
  last    = 1,
  default_= 2
}

enum TimeUnit : ubyte {
  SECONDS = 0,
  MILLIS  = 1,
  MICROS  = 2,
  NANOS   = 3
}

enum BasicSqlDataFilterType : ubyte {
  equals              = 0,
  notEquals           = 1,
  lessThan            = 2,
  lessThanOrEquals    = 3,
  greaterThan         = 4,
  greaterThanOrEquals = 5,

  isNull              = 6,
  isNotNull           = 7,

  regex               = 8,
  notRegex            = 9,
  startsWith          = 10,
  endsWith            = 11,
  contains            = 12,
  notContains         = 13,

  arrayContains       = 14,
  arrayContainedBy    = 15,
  arrayOverlaps       = 16,
  arrayEquals         = 17,
  arrayNotEquals      = 18,
  arrayEmpty          = 19,
  arrayNotEmpty       = 20,
  arrayLength         = 21,

  jsonContains        = 22,
  jsonContainedBy     = 23,
  jsonHasKey          = 24,
  jsonHasAnyKey       = 25,
  jsonHasAllKeys      = 26,
  jsonGetField        = 27,
  jsonGetFieldAsText  = 28,

  between             = 29,
  notBetween          = 30,
  rangeContains       = 31,
  rangeContainedBy    = 32,

  inList              = 33,
  notInList           = 34
}

enum OrderSort : ubyte {
  ASC_DEFAULT      = 0,
  ASC_NULLS_FIRST  = 1,
  ASC_NULLS_LAST   = 2,
  DESC_DEFAULT     = 3,
  DESC_NULLS_FIRST = 4,
  DESC_NULLS_LAST  = 5
}

// ---------------- Helper tables (unchanged) ----------------
table KeyValuePair { key: string; value: string; }
table CustomOptions { options: [KeyValuePair]; }
table TableOptions  { options: [KeyValuePair]; }

table DataHelperAggregation {
  count: uint32;
  sum_values: [KeyValuePair];
  avg_values: [KeyValuePair];
  minimum_values: [KeyValuePair];
  maximum_values: [KeyValuePair];
}

table DataSort { field: string; ascending: bool; }

// ---------------- Value tables ----------------
table StringValue   { value: string; }
table NumberValue   { value: double; }
table Int64Value    { value: long; }
table BoolValue     { value: bool; }
table NullValue     { }

table TimestampValue {
  epoch: long;
  unit:  TimeUnit = MICROS;
}

table StringList  { values:[string] (required); }
table Int64List   { values:[long]   (required); }
table Float64List { values:[double] (required); }
table BoolList    { values:[bool]   (required); }

// ---------------- Unions ----------------
union FilterValue {
  StringValue,
  NumberValue,
  BoolValue,
  NullValue,
  Int64Value,
  TimestampValue,
  StringList,
  Int64List,
  Float64List,
  BoolList
}

table RangeValue {
  low:  FilterValue;
  high: FilterValue;
  include_low:  bool = true;
  include_high: bool = true;
}

table CursorEntry { field:string (required); value:FilterValue; }

// ---------------- Filters ----------------
table SqlFilterModifier {
  distinct: bool;
  case_insensitive: bool;
  nulls_order: NullsSortOrder;
}

table BasicSqlDataFilter {
  field_name: string;
  value: FilterValue;
  filter_type: BasicSqlDataFilterType;
  modifier: SqlFilterModifier;
}

union BasicSqlDataFilterUnion { BasicSqlDataFilterWrapper, BasicSqlDataFilter }

table BasicSqlDataFilterWrapper {
  filter_wrapper_type: SQLFilterWrapperType;
  filters: [BasicSqlDataFilterUnion];
}

// ---------------- Query ----------------
table OrderKeySpec {
  field:string (required);
  sort:OrderSort = DESC_DEFAULT;
  is_pk:bool = false;
}

table StreamingSqlDataFilter {
  hash: string;
  wrapper: BasicSqlDataFilterWrapper;
  limit: uint32 = 50;
  order: [OrderKeySpec];
  cursor: [CursorEntry];
  schema_version: ushort = 1;
}

// ---------------- Table metadata ----------------
table TableColumn {
  name: string;
  data_type: SQLDataType;
  is_nullable: bool;
  constraints: [ColumnConstraint];
  custom_options: CustomOptions;
}

table TableDefinition {
  name: string;
  columns: [TableColumn];
  comment: string;
  table_options: TableOptions;
}

root_type StreamingSqlDataFilter;

-------- [ Separator ] ------

File Name: src/index.ts
Size: 162 B
Code:
import { startRpcWorker } from "./rpc/worker";

startRpcWorker().catch((err) => {
    console.error("RPC worker failed to start:", err);
    process.exit(1);
});

-------- [ Separator ] ------

File Name: src/rpc/fb_decode.ts
Size: 4.64 KB
Code:
// packages/postgres_rpc/src/rpc/fb_decode.ts
import {
    OrderKeySpec,
    OrderSort,
    SqlDataFilter,
    SqlDataFilterWrapper,
    SQLDataFilterType,
    SQLFilterWrapperType,
    SqlFilterModifier,
    NullsSortOrder,
} from "@cladbe/postgres_manager/dist/models/filters/filters";
import { orderSortTokenFromOrdinal } from "./fb_maps";

// Adjust this import to match where flatc put your TS outputs
import * as FBG from "./generated/sql_rpc";

function toOrderSort(ord: number): OrderSort {
    return orderSortTokenFromOrdinal(ord) as unknown as OrderSort;
}

export function decodeOrderKey(ok: any): OrderKeySpec {
    const field = ok.field() as string;
    const sort = toOrderSort(Number(ok.sort?.() ?? ok.sort));
    const isPk = (ok.isPk?.() ?? false);
    return { field, sort, };
}

export function decodeWrapper(w: any): SqlDataFilterWrapper {
    const filters: (SqlDataFilter | SqlDataFilterWrapper)[] = [];
    for (let i = 0; i < w.filtersLength(); i++) {
        const utype = w.filtersType(i);
        if (utype === FBG.SqlSchema.BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper) {
            const childW = new FBG.SqlSchema.BasicSqlDataFilterWrapper();
            w.filters(childW, i);
            filters.push(decodeWrapper(childW));
        } else {
            const childL = new FBG.SqlSchema.BasicSqlDataFilter();
            w.filters(childL, i);
            filters.push(decodeLeaf(childL));
        }
    }
    const wrapperType: SQLFilterWrapperType =
        w.filterWrapperType() === FBG.SqlSchema.SQLFilterWrapperType.and
            ? SQLFilterWrapperType.and
            : SQLFilterWrapperType.or;

    return { filterWrapperType: wrapperType, filters };
}

function decodeLeaf(lf: any): SqlDataFilter {
    const fieldName = lf.fieldName() as string;

    // Build a *typed* modifier so nullsOrder is NullsSortOrder (not string)
    const modifierFB = lf.modifier?.();
    const modifier: SqlFilterModifier = {
        distinct: !!(modifierFB?.distinct?.() ?? false),
        caseInSensitive: !!(modifierFB?.caseInsensitive?.() ?? false),
        nullsOrder: NullsSortOrder.default_,
    };

    const filterTypeOrdinal = Number(lf.filterType?.() ?? lf.filterType);
    const filterType = mapFilterType(filterTypeOrdinal);
    const value = getFilterValue(lf);

    return { fieldName, value, filterType, modifier };
}

function mapFilterType(ord: number): SQLDataFilterType {
    const T = FBG.SqlSchema.BasicSqlDataFilterType;
    switch (ord) {
        case T.equals:       return SQLDataFilterType.equals;
        case T.notEquals:    return SQLDataFilterType.notEquals;
        case T.isNull:       return SQLDataFilterType.isNull;
        case T.isNotNull:    return SQLDataFilterType.isNotNull;
        case T.startsWith:   return SQLDataFilterType.startsWith;
        case T.endsWith:     return SQLDataFilterType.endsWith;
        case T.contains:     return SQLDataFilterType.contains;
        case T.between:      return SQLDataFilterType.between;
        case T.notBetween:   return SQLDataFilterType.notBetween;
        case T.inList:       return SQLDataFilterType.in_;
        case T.notInList:    return SQLDataFilterType.notIn;
        default:
            throw new Error(`Unsupported filter type ordinal: ${ord}`);
    }
}

function getFilterValue(lf: any): any {
    const vt = lf.valueType();
    const V = FBG.SqlSchema.FilterValue;

    if (vt === V.NullValue) return null;

    if (vt === V.StringValue)  { const v = new FBG.SqlSchema.StringValue();  lf.value(v); return v.value(); }
    if (vt === V.NumberValue)  { const v = new FBG.SqlSchema.NumberValue();  lf.value(v); return v.value(); }
    if (vt === V.Int64Value)   { const v = new FBG.SqlSchema.Int64Value();   lf.value(v); return Number(v.value()); }
    if (vt === V.BoolValue)    { const v = new FBG.SqlSchema.BoolValue();    lf.value(v); return v.value(); }

    if (vt === V.StringList)   { const v = new FBG.SqlSchema.StringList();   lf.value(v); return collectList(v.valuesLength(), i => v.values(i)); }
    if (vt === V.Int64List)    { const v = new FBG.SqlSchema.Int64List();    lf.value(v); return collectList(v.valuesLength(), i => Number(v.values(i))); }
    if (vt === V.Float64List)  { const v = new FBG.SqlSchema.Float64List();  lf.value(v); return collectList(v.valuesLength(), i => v.values(i)); }

    // Add TimestampValue/RangeValue mapping later if you start using them
    throw new Error(`Unsupported filter value type: ${vt}`);
}

function collectList<T>(len: number, getter: (i: number) => T | null | undefined): T[] {
    const out: T[] = [];
    for (let i = 0; i < len; i++) {
        const val = getter(i);
        if (val !== undefined && val !== null) out.push(val as T);
    }
    return out;
}

-------- [ Separator ] ------

File Name: src/rpc/fb_maps.ts
Size: 656 B
Code:
// packages/postgres_rpc/src/rpc/fb_maps.ts
export type OrderSortToken =
    | "ASC_DEFAULT" | "ASC_NULLS_FIRST" | "ASC_NULLS_LAST"
    | "DESC_DEFAULT" | "DESC_NULLS_FIRST" | "DESC_NULLS_LAST";

export type OrderSortOrdinal = 0 | 1 | 2 | 3 | 4 | 5;

export function orderSortTokenFromOrdinal(ord: number): OrderSortToken {
    switch (ord as OrderSortOrdinal) {
        case 0: return "ASC_DEFAULT";
        case 1: return "ASC_NULLS_FIRST";
        case 2: return "ASC_NULLS_LAST";
        case 3: return "DESC_DEFAULT";
        case 4: return "DESC_NULLS_FIRST";
        case 5: return "DESC_NULLS_LAST";
        default: return "DESC_DEFAULT";
    }
}

-------- [ Separator ] ------

File Name: src/rpc/kafka.ts
Size: 3.68 KB
Code:
// src/rpc/kafka.ts
/* eslint-disable @typescript-eslint/no-explicit-any */
import {
    KafkaConsumer,
    Producer,
    LibrdKafkaError,
    Message,
} from "node-rdkafka";

export type KafkaConfig = {
    brokers: string[];            // ["localhost:9092"]
    groupId: string;              // "cladbe-postgres-rpc"
    requestTopic: string;         // "sql.rpc.requests"
};

type OnMessage = (msg: Message) => void;

export class RpcKafka {
    private producer: Producer;
    private consumer: KafkaConsumer;
    private pollTimer?: NodeJS.Timeout;
    private onMessage?: OnMessage;

    constructor(private cfg: KafkaConfig) {
        this.producer = new Producer(
            {
                "metadata.broker.list": cfg.brokers.join(","),
                "client.id": "cladbe-postgres-rpc",
                "socket.keepalive.enable": true,
                // delivery reports disabled for simplicity; we poll to drain queue
                "dr_cb": false,
            },
            {}
        );

        this.consumer = new KafkaConsumer(
            {
                "metadata.broker.list": cfg.brokers.join(","),
                "group.id": cfg.groupId,
                "enable.auto.commit": true,
                "socket.keepalive.enable": true,
                "allow.auto.create.topics": true,
                "client.id": "cladbe-postgres-rpc",
            },
            { "auto.offset.reset": "latest" }
        );
    }

    setHandler(onMessage: OnMessage) {
        this.onMessage = onMessage;
    }

    async start() {
        // connect producer first
        await new Promise<void>((resolve, reject) => {
            this.producer
                .on("ready", () => resolve())
                .on("event.error", (err: LibrdKafkaError) => reject(err));
            this.producer.connect();
        });

        // drain internal queue regularly
        this.pollTimer = setInterval(() => {
            try { this.producer.poll(); } catch { /* ignore */ }
        }, 100);

        // then consumer
        await new Promise<void>((resolve, _reject) => {
            this.consumer
                .on("ready", () => {
                    this.consumer.subscribe([this.cfg.requestTopic]);
                    this.consumer.consume();
                    resolve();
                })
                .on("data", (m: Message) => {
                    if (!m.value) return;
                    this.onMessage?.(m);
                })
                .on("event.error", (err: LibrdKafkaError) => {
                    console.error("[rpc] consumer error", err);
                })
                .on("rebalance", (ev: unknown) => {
                    console.log("[rpc] rebalance", ev);
                });

            this.consumer.connect();
        });
    }

    stop() {
        if (this.pollTimer) clearInterval(this.pollTimer);
        try { this.consumer.disconnect(); } catch {}
        try { this.producer.disconnect(); } catch {}
    }

    /** resilient produce with small retry when librdkafka queue is full */
    produceSafe(topic: string, key: string, value: Buffer, attempt = 0) {
        try {
            this.producer.produce(topic, null, value, key);
        } catch (e: any) {
            const msg = String(e?.message || e);
            const queueFull =
                e?.code === -184 /* RD_KAFKA_RESP_ERR__QUEUE_FULL */ ||
                msg.toLowerCase().includes("queue");
            if (queueFull && attempt < 10) {
                this.producer.poll();
                setTimeout(
                    () => this.produceSafe(topic, key, value, attempt + 1),
                    25
                );
                return;
            }
            console.error("[rpc] produce failed", e);
        }
    }
}
-------- [ Separator ] ------

File Name: src/rpc/worker.ts
Size: 9.98 KB
Code:
// src/rpc/worker.ts
/* eslint-disable @typescript-eslint/no-explicit-any */
import dotenv from "dotenv";
import * as flatbuffers from "flatbuffers";
import type { Message } from "node-rdkafka";

import { PostgresManager } from "@cladbe/postgres_manager";
import {
    GetDataDbRequest,
    GetSingleRecordRequest,
    AddSingleDbRequest,
    UpdateSingleDbRequest,
    DeleteRowDbRequest,
    AggregationRequest,
    TableExistsRequest,
} from "@cladbe/postgres_manager/dist/models/requests";

import * as FBG from "./generated/sql_rpc";
import { decodeOrderKey, decodeWrapper } from "./fb_decode";
import { RpcKafka } from "./kafka";

dotenv.config();

const REQUEST_TOPIC = process.env.SQL_RPC_REQUEST_TOPIC || "sql.rpc.requests";
const RESPONSE_TOPIC_DEFAULT =
    process.env.SQL_RPC_RESPONSE_TOPIC || "sql.rpc.responses";
const BROKERS = (process.env.KAFKA_BROKERS || "localhost:9092").split(",");
const GROUP_ID = process.env.SQL_RPC_GROUP_ID || "cladbe-postgres-rpc";

const pg = PostgresManager.getInstance();
const kafka = new RpcKafka({
    brokers: BROKERS,
    groupId: GROUP_ID,
    requestTopic: REQUEST_TOPIC,
});

// ----- FB response builders -----
function okRows(corr: string, rows: any[]): Buffer {
    const b = new flatbuffers.Builder(1024);
    const rowOffsets = rows.map((r) => b.createString(JSON.stringify(r)));
    const rowsVec = FBG.SqlRpc.RowsJson.createRowsVector(b, rowOffsets);
    const rowsObj = FBG.SqlRpc.RowsJson.createRowsJson(b, rowsVec);
    const env = FBG.SqlRpc.ResponseEnvelope.createResponseEnvelope(
        b,
        b.createString(corr),
        true,
        FBG.SqlRpc.ErrorCode.NONE,
        0,
        FBG.SqlRpc.RpcResponse.RowsJson,
        rowsObj
    );
    b.finish(env);
    return Buffer.from(b.asUint8Array());
}
function okRow(corr: string, row: any): Buffer {
    const b = new flatbuffers.Builder(512);
    const rowStr = FBG.SqlRpc.RowJson.createRowJson(
        b,
        b.createString(JSON.stringify(row))
    );
    const env = FBG.SqlRpc.ResponseEnvelope.createResponseEnvelope(
        b,
        b.createString(corr),
        true,
        FBG.SqlRpc.ErrorCode.NONE,
        0,
        FBG.SqlRpc.RpcResponse.RowJson,
        rowStr
    );
    b.finish(env);
    return Buffer.from(b.asUint8Array());
}
function okBool(corr: string, val: boolean): Buffer {
    const b = new flatbuffers.Builder(64);
    const boolRes = FBG.SqlRpc.BoolRes.createBoolRes(b, val);
    const env = FBG.SqlRpc.ResponseEnvelope.createResponseEnvelope(
        b,
        b.createString(corr),
        true,
        FBG.SqlRpc.ErrorCode.NONE,
        0,
        FBG.SqlRpc.RpcResponse.BoolRes,
        boolRes
    );
    b.finish(env);
    return Buffer.from(b.asUint8Array());
}
function err(corr: string, code: number, message: string): Buffer {
    const b = new flatbuffers.Builder(256);
    const env = FBG.SqlRpc.ResponseEnvelope.createResponseEnvelope(
        b,
        b.createString(corr),
        false,
        code,
        b.createString(message),
        0,
        0
    );
    b.finish(env);
    return Buffer.from(b.asUint8Array());
}

// ----- helpers -----
function collect(m: any, base: string): string[] | undefined {
    const len = m[`${base}Length`]?.() ?? 0;
    if (!len) return undefined;
    const out: string[] = [];
    for (let i = 0; i < len; i++) out.push(m[base](i)!);
    return out;
}
function readCursorValue(ce: any): any {
    const vt = ce.valueType();
    const V = FBG.SqlSchema.FilterValue;
    if (vt === V.NullValue) return null;
    if (vt === V.StringValue) { const v = new FBG.SqlSchema.StringValue();  ce.value(v); return v.value(); }
    if (vt === V.NumberValue) { const v = new FBG.SqlSchema.NumberValue();  ce.value(v); return v.value(); }
    if (vt === V.Int64Value)  { const v = new FBG.SqlSchema.Int64Value();   ce.value(v); return Number(v.value()); }
    if (vt === V.BoolValue)   { const v = new FBG.SqlSchema.BoolValue();     ce.value(v); return v.value(); }
    if (vt === V.TimestampValue) { const v = new FBG.SqlSchema.TimestampValue(); ce.value(v); return Number(v.epoch()); }
    return undefined;
}

// ----- request handler -----
async function handleEnv(envBytes: Uint8Array) {
    const bb = new flatbuffers.ByteBuffer(envBytes);
    const req = FBG.SqlRpc.RequestEnvelope.getRootAsRequestEnvelope(bb);

    const corr = req.correlationId() || "";
    const replyTopic = req.replyTopic() || RESPONSE_TOPIC_DEFAULT;

    try {
        switch (req.method()) {
            case FBG.SqlRpc.RpcMethod.GET_DATA: {
                const m = new FBG.SqlRpc.GetDataReq(); req.payload(m);

                const order: any[] = [];
                for (let i = 0; i < m.orderLength(); i++) {
                    const ok = new FBG.SqlSchema.OrderKeySpec(); m.order(i, ok);
                    order.push(decodeOrderKey(ok));
                }
                const wrapper = m.wrapper();
                const filters = wrapper ? [decodeWrapper(wrapper)] : undefined;

                const cursor: Record<string, any> | undefined = (() => {
                    const len = (m.cursorLength && m.cursorLength()) || 0;
                    if (!len) return undefined;
                    const obj: Record<string, any> = {};
                    for (let i = 0; i < len; i++) {
                        const ce = m.cursor(i)!;
                        obj[ce.field()!] = readCursorValue(ce);
                    }
                    return obj;
                })();

                const strictAfter = (m.strictAfter && m.strictAfter()) ?? true;

                const tsReq = new GetDataDbRequest(
                    m.tableName()!,
                    m.companyId()!,
                    undefined,
                    filters,
                    m.limit() || undefined,
                    m.offset() || undefined,
                    order,
                    cursor,
                    strictAfter
                );
                const rows = await pg.getData(tsReq);
                kafka.produceSafe(replyTopic, corr, okRows(corr, rows as any[]));
                break;
            }

            case FBG.SqlRpc.RpcMethod.GET_SINGLE: {
                const m = new FBG.SqlRpc.GetSingleReq(); req.payload(m);
                const tsReq = new GetSingleRecordRequest(
                    m.tableName()!, m.companyId()!, m.primaryKeyColumn()!, m.primaryId()!
                );
                const row = await pg.getData(tsReq);
                kafka.produceSafe(replyTopic, corr, okRow(corr, row));
                break;
            }

            case FBG.SqlRpc.RpcMethod.ADD_SINGLE: {
                const m = new FBG.SqlRpc.AddSingleReq(); req.payload(m);
                const tsReq = new AddSingleDbRequest(
                    m.tableName()!, m.companyId()!, m.primaryKeyColumn()!, JSON.parse(m.rowJson()!)
                );
                const rows = await pg.editData(tsReq);
                kafka.produceSafe(replyTopic, corr, okRows(corr, rows as any[]));
                break;
            }

            case FBG.SqlRpc.RpcMethod.UPDATE_SINGLE: {
                const m = new FBG.SqlRpc.UpdateSingleReq(); req.payload(m);
                const tsReq = new UpdateSingleDbRequest(
                    m.tableName()!, m.companyId()!, JSON.parse(m.updatesJson()!),
                    m.primaryKeyColumn()!, m.primaryId()!
                );
                const rows = await pg.editData(tsReq);
                kafka.produceSafe(replyTopic, corr, okRows(corr, rows as any[]));
                break;
            }

            case FBG.SqlRpc.RpcMethod.DELETE_ROW: {
                const m = new FBG.SqlRpc.DeleteRowReq(); req.payload(m);
                const rows = await pg.deleteRequest(new DeleteRowDbRequest({
                    tableName: m.tableName()!, companyId: m.companyId()!,
                    primaryKeyColumn: m.primaryKeyColumn()!, primaryId: m.primaryId()!,
                }));
                kafka.produceSafe(replyTopic, corr, okRows(corr, rows as any[]));
                break;
            }

            case FBG.SqlRpc.RpcMethod.TABLE_EXISTS: {
                const m = new FBG.SqlRpc.TableExistsReq(); req.payload(m);
                const exists = await pg.tableExists(new TableExistsRequest(m.tableName()!, m.companyId()!));
                kafka.produceSafe(replyTopic, corr, okBool(corr, !!exists));
                break;
            }

            case FBG.SqlRpc.RpcMethod.RUN_AGGREGATION: {
                const m = new FBG.SqlRpc.RunAggregationReq(); req.payload(m);
                const wrapper = m.wrapper();
                const filters = wrapper ? [decodeWrapper(wrapper)] : undefined;
                const agg = await pg.runAggregationQuery(new AggregationRequest({
                    tableName: m.tableName()!, companyId: m.companyId()!,
                    sumFields: collect(m, "sumFields"),
                    averageFields: collect(m, "averageFields"),
                    minimumFields: collect(m, "minimumFields"),
                    maximumFields: collect(m, "maximumFields"),
                    countEnabled: m.countEnabled(),
                    filters,
                }));
                kafka.produceSafe(replyTopic, corr, okRow(corr, agg));
                break;
            }

            case FBG.SqlRpc.RpcMethod.CREATE_TABLE: {
                throw new Error("CREATE_TABLE via RPC not implemented; use HTTP route for now.");
            }

            default:
                throw new Error(`Unknown RPC method: ${req.method()}`);
        }
    } catch (e: any) {
        kafka.produceSafe(
            replyTopic,
            corr,
            err(corr, FBG.SqlRpc.ErrorCode.INTERNAL, e?.message || "internal")
        );
    }
}

// ---- lifecycle ----
export async function startRpcWorker() {
    kafka.setHandler((msg: Message) => {
        if (!msg.value) return;
        void handleEnv(new Uint8Array(msg.value as Buffer));
    });

    await kafka.start();

    const shutdown = () => kafka.stop();
    process.on("SIGINT", shutdown);
    process.on("SIGTERM", shutdown);

    console.log(`[RPC] listening: ${REQUEST_TOPIC} -> replies to ${RESPONSE_TOPIC_DEFAULT} (rdkafka; refactored)`);
}
-------- [ Separator ] ------

File Name: src/types/env.d.ts
Size: 364 B
Code:
declare namespace NodeJS {
    interface ProcessEnv {
        KAFKA_BROKERS?: string;             // "localhost:9092,localhost:9093"
        SQL_RPC_REQUEST_TOPIC?: string;     // default: sql.rpc.requests
        SQL_RPC_RESPONSE_TOPIC?: string;    // default: sql.rpc.responses
        SQL_RPC_GROUP_ID?: string;          // default: cladbe-postgres-rpc
    }
}

-------- [ Separator ] ------
